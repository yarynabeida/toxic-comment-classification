# toxic-comment-classification

This project is an educational endeavor that guides you through fine-tuning a transformer model for multilabel text classification. The project is organized into distinct steps, including importing necessary libraries, preprocessing domain-specific data, setting up datasets and dataloaders, creating a neural network for fine-tuning, executing the fine-tuning process, and evaluating model performance.

To facilitate this, the Jigsaw toxic comment dataset from Kaggle [Toxic Comment Competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) is employed. The dataset consists of comment texts marked with labels like `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, and `identity_hate`. The model architecture chosen for fine-tuning is `BERT`, a powerful transformer model developed by Google AI.

This practical project equips learners with hands-on experience in the realms of multilabel text classification and transformer-based NLP models.
